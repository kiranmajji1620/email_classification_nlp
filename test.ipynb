{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wrrPCa9F7Arz"
      },
      "source": [
        "#Project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4klbzUFz7DoD",
        "outputId": "8539777b-bd23-418a-a0ae-4db585b9c436"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\python311\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\python311\\lib\\site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\python311\\lib\\site-packages (from nltk) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in c:\\python311\\lib\\site-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\sai kiran\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\SAI\n",
            "[nltk_data]     KIRAN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to C:\\Users\\SAI\n",
            "[nltk_data]     KIRAN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to C:\\Users\\SAI\n",
            "[nltk_data]     KIRAN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OgIv7LfJb6qN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LJyDvi3b_XP",
        "outputId": "339591e6-fdf2-476f-ac51-40dd2ed0e462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                 text       label\n",
            "0   Greetings from GDSC\\nI hope everyone is enjoyi...  Internship\n",
            "1   Dear SirMadam\\nSRISHTI23 the summer research i...  Internship\n",
            "2   Dear learner Here is an exciting announcement ...  Internship\n",
            "3   Greetings from Alumni Affairs IIITDM Please fi...  Internship\n",
            "4   Dear Binsu I hope this email finds you well We...  Internship\n",
            "5   Dear Students Helsi Green Equipment Manufactur...  Internship\n",
            "6   Dear Community Introducing the Cricket Code Ch...  Internship\n",
            "7   Discussion Set 4\\nDear students PFA the Discus...  Assignment\n",
            "8   Assignment\\n1 Handwritten assignment should be...  Assignment\n",
            "9   Dear Students Illustrate the modulation and de...  Assignment\n",
            "10  Dear Learner The solution of Assignment 2 has ...  Assignment\n",
            "11  Dear students The lecture videos for Week1 hav...  Assignment\n",
            "12  Discussion Session 1\\nDear Students We will ha...  Assignment\n",
            "13  Dear all Plenty of time has been given for sub...  Assignment\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"text_data.csv\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wWqSRi3-cEom"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    preprocessed_text = ' '.join(words)\n",
        "    return preprocessed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VYbVlGUwcLNV"
      },
      "outputs": [],
      "source": [
        "df[\"preprocessed_text\"] = df[\"text\"].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MSw0ZtF5cO94"
      },
      "outputs": [],
      "source": [
        "X, y = df[\"preprocessed_text\"], df['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TygqX-l9dzU1"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "q7Lct2ADcYog",
        "outputId": "36158354-d366-4cda-c2c7-b20067209aa9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_vectorized, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krNwqfYscXt-",
        "outputId": "a8c4a54b-1428-4586-ee73-803f4ad9f603"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "  Assignment       1.00      1.00      1.00         2\n",
            "  Internship       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         3\n",
            "   macro avg       1.00      1.00      1.00         3\n",
            "weighted avg       1.00      1.00      1.00         3\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred = clf.predict(X_test_vectorized)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", report)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "imGO_VrSb1LJ"
      },
      "source": [
        "#countVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zW_4Qk4jb-2H",
        "outputId": "94625aa6-6125-4300-8764-74dc3eac5887"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document Term Frequency: \n",
            "    and  are  bain  because  boston  career  company  consultants  consulting  \\\n",
            "0    0    0     0        1       0       0        0            0           0   \n",
            "1    1    1     1        0       1       1        1            1           2   \n",
            "\n",
            "   cv  ...  preparation  rated  review  roles  sai  science  subject  to  \\\n",
            "0   0  ...            0      0       0      0    0        1        1   0   \n",
            "1   1  ...            1      1       1      1    1        0        0   1   \n",
            "\n",
            "   with  your  \n",
            "0     0     0  \n",
            "1     1     1  \n",
            "\n",
            "[2 rows x 39 columns]\n",
            "  (0, 1)\t1\n",
            "  (0, 19)\t1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sen0 = \"data science is my favourite subject because data is oil\"\n",
        "sen1 = \"\"\"Hi Sai, Highly rated consultants from McKinsey, Boston Consulting Group, Bain & Company, Kearney etc are\n",
        "here to help out with your dream consulting roles including CV Review, Career Guidance, and Interview Preparation!\"\"\"\n",
        "sen2 = \"hi how are you\"\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform([sen0, sen1])\n",
        "X_new = vectorizer.transform([sen2])\n",
        "words = vectorizer.get_feature_names_out()\n",
        "df = pd.DataFrame(data=X.toarray(), columns=words)\n",
        "print(\"Document Term Frequency: \\n\", df)\n",
        "print(X_new)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5jR2OXIlffGN"
      },
      "source": [
        "# Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsO1ibBVl7iM",
        "outputId": "454bdbb8-bc6a-4afc-f972-ea74053a3a08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "into\n",
            "ma\n",
            "its\n",
            "what\n",
            "am\n",
            "when\n",
            "are\n",
            "they\n",
            "been\n",
            "aren\n",
            "all\n",
            "them\n",
            "themselves\n",
            "having\n",
            "if\n",
            "in\n",
            "ours\n",
            "is\n",
            "needn\n",
            "same\n",
            "myself\n",
            "those\n",
            "where\n",
            "m\n",
            "o\n",
            "here\n",
            "isn't\n",
            "we\n",
            "d\n",
            "ll\n",
            "who\n",
            "on\n",
            "from\n",
            "weren\n",
            "do\n",
            "hadn't\n",
            "both\n",
            "own\n",
            "will\n",
            "up\n",
            "down\n",
            "there\n",
            "haven\n",
            "she\n",
            "as\n",
            "can\n",
            "should\n",
            "some\n",
            "the\n",
            "off\n",
            "under\n",
            "their\n",
            "why\n",
            "other\n",
            "itself\n",
            "mustn\n",
            "didn\n",
            "you're\n",
            "it's\n",
            "for\n",
            "he\n",
            "than\n",
            "won't\n",
            "a\n",
            "at\n",
            "it\n",
            "through\n",
            "couldn't\n",
            "this\n",
            "haven't\n",
            "most\n",
            "not\n",
            "theirs\n",
            "did\n",
            "was\n",
            "don\n",
            "against\n",
            "of\n",
            "yourself\n",
            "such\n",
            "wasn\n",
            "which\n",
            "nor\n",
            "each\n",
            "re\n",
            "don't\n",
            "doesn't\n",
            "hers\n",
            "have\n",
            "yours\n",
            "hasn't\n",
            "shouldn\n",
            "herself\n",
            "how\n",
            "that'll\n",
            "below\n",
            "didn't\n",
            "whom\n",
            "again\n",
            "wouldn\n",
            "before\n",
            "himself\n",
            "any\n",
            "you\n",
            "our\n",
            "because\n",
            "mightn\n",
            "an\n",
            "ourselves\n",
            "had\n",
            "very\n",
            "between\n",
            "couldn\n",
            "during\n",
            "weren't\n",
            "ain\n",
            "doesn\n",
            "does\n",
            "him\n",
            "shouldn't\n",
            "or\n",
            "were\n",
            "then\n",
            "i\n",
            "once\n",
            "after\n",
            "and\n",
            "needn't\n",
            "should've\n",
            "my\n",
            "that\n",
            "few\n",
            "only\n",
            "hadn\n",
            "hasn\n",
            "be\n",
            "t\n",
            "about\n",
            "ve\n",
            "isn\n",
            "you'd\n",
            "no\n",
            "her\n",
            "mightn't\n",
            "me\n",
            "you've\n",
            "doing\n",
            "wasn't\n",
            "while\n",
            "shan't\n",
            "out\n",
            "aren't\n",
            "wouldn't\n",
            "being\n",
            "won\n",
            "but\n",
            "y\n",
            "your\n",
            "now\n",
            "so\n",
            "just\n",
            "with\n",
            "by\n",
            "more\n",
            "over\n",
            "s\n",
            "mustn't\n",
            "his\n",
            "these\n",
            "shan\n",
            "further\n",
            "she's\n",
            "you'll\n",
            "to\n",
            "too\n",
            "yourselves\n",
            "until\n",
            "above\n",
            "has\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\SAI\n",
            "[nltk_data]     KIRAN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "for word in stop_words:\n",
        "    print(word)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JyvA_Q16mtwx"
      },
      "source": [
        "#Lemmetization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZF8OLGMm8yg",
        "outputId": "b917828f-ae88-44c4-d71a-f95b4bd4ac71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running-> running\n",
            "flies-> fly\n",
            "better-> better\n",
            "cats-> cat\n",
            "better-> better\n",
            "go-> go\n",
            "going-> going\n",
            "goes-> go\n",
            "went-> went\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to C:\\Users\\SAI\n",
            "[nltk_data]     KIRAN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "word_list = [\"running\", \"flies\", \"better\", \"cats\", \"better\", \"go\", \"going\", \"goes\", \"went\"]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in word_list]\n",
        "\n",
        "\n",
        "for i, j in zip(word_list, lemmatized_words):\n",
        "    print(i + \"->\", j)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
